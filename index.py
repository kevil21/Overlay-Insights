import os
import csv
import sys
import logging
import dotenv
from pathlib import Path
from typing import Iterator, Dict, Any
from elasticsearch import Elasticsearch, helpers

# ---------- Config ----------
CSV_PATH = Path("data/IncidentTableData_cleaned.csv")
INDEX_NAME = "incident_tabledata"

# Elastic Cloud endpoint + API key
dotenv.load_dotenv()  # load from .env file
ES_ENDPOINT = os.getenv("ES_ENDPOINT")
ES_API_KEY = os.getenv("ES_API_KEY")


logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("index_data")

# ---------- Mappings ----------
MAPPINGS = {
    "mappings": {
        "properties": {
            "ticket_class": {"type": "keyword"},
            "ticket_priority": {"type": "integer"},
            "ticket_number": {"type": "keyword"},
            "ticket_status": {"type": "keyword"},
            "opened_date": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||strict_date_optional_time"},
            "hostname": {"type": "keyword"},
            "ticket_summary": {"type": "text"},
            "queue_id": {"type": "keyword"},
            "ticket_resolved_date": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||strict_date_optional_time"},
            "resolution_code": {"type": "keyword"},
            "resolution_text": {"type": "text"},
            "ticket_closed_date": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||strict_date_optional_time"},
            "call_code": {"type": "keyword"},
            "reported_by": {"type": "keyword"},
            "executed_automata": {"type": "keyword"},
            "recommended_automata": {"type": "keyword"},
            "actionable": {"type": "keyword"},
            "assignment_queue": {"type": "keyword"},
            "autogenerated": {"type": "keyword"},
            "automation_engine": {"type": "integer"},
            "business_application": {"type": "keyword"},
            "closure_code": {"type": "keyword"},
            "sub_category": {"type": "keyword"},
            "category": {"type": "keyword"},
            "alert_key": {"type": "keyword"},
            "opened_year": {"type": "integer"},
            "opened_month": {"type": "integer"},
            "opened_day": {"type": "integer"},
            "opened_hour": {"type": "integer"},
            "resolution_time_hours": {"type": "float"},
            "ticket_age_days": {"type": "integer"},
            "is_automated": {"type": "boolean"},
            "critical_application_flag": {"type": "boolean"}
        }
    }
}

def get_client() -> Elasticsearch:
    es = Elasticsearch(
        ES_ENDPOINT,
        api_key=ES_API_KEY,
        request_timeout=120,
        retry_on_timeout=True,
        max_retries=3,
        verify_certs=True,
    )
    try:
        if not es.ping():
            logger.warning("Elasticsearch ping failed; check endpoint/API key.")
    except Exception as e:
        logger.warning("Ping raised an exception: %s", e)
    return es

def ensure_index(es: Elasticsearch, index_name: str) -> None:
    """
    Create index with explicit mappings if it doesn't exist.
    """
    try:
        exists = es.indices.exists(index=index_name)
    except Exception:
        exists = False

    if not exists:
        logger.info("Creating index '%s' with explicit mappings...", index_name)
        es.indices.create(index=index_name, body=MAPPINGS)
    else:
        logger.info("Index '%s' already exists; skipping creation.", index_name)

def read_csv_rows(path: Path) -> Iterator[Dict[str, Any]]:
    """
    Stream rows from CSV as dicts (CSV to JSON conversion).
    Assumes first line is header.
    """
    with path.open(newline="", encoding="utf-8") as fh:
        reader = csv.DictReader(fh, delimiter='\t')
        for row in reader:
            yield row

def normalize_row(row: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert 'Unknown' (any case/whitespace) and empty strings to None.
    Convert TRUE/FALSE strings to Python booleans.
    Convert numeric fields to correct types.
    """
    out = {}
    for k, v in row.items():
        if isinstance(v, str):
            v_clean = v.strip()
            v_lower = v_clean.lower()
            if v_lower == "unknown" or v_clean == "":
                out[k] = None
            elif v_lower == "true":
                out[k] = True
            elif v_lower == "false":
                out[k] = False
            else:
                if k in {"ticket_priority", "opened_year", "opened_month", "opened_day", "opened_hour", "ticket_age_days"}:
                    try:
                        out[k] = int(v_clean)
                    except Exception:
                        out[k] = None
                elif k in {"resolution_time_hours"}:
                    try:
                        out[k] = float(v_clean)
                    except Exception:
                        out[k] = None
                else:
                    out[k] = v_clean
        else:
            out[k] = v
    return out

def actions_from_rows(rows: Iterator[Dict[str, Any]], index_name: str):
    """
    Turning CSV rows into bulk index actions.
    Use 'ticket_number' as _id for records.
    """
    for row in rows:
        row = normalize_row(row)
        doc_id = None

        for key in ("ticket_number", "Ticket Number", "ticketnumber"):
            if key in row and (row[key] or "").strip():
                doc_id = str(row[key]).strip()
                break
        if not doc_id:
            for key in ("hostname", "Hostname"):
                if key in row and (row[key] or "").strip():
                    doc_id = str(row[key]).strip()
                    break

        yield {
            "_op_type": "index",
            "_index": index_name,
            "_id": doc_id,
            "_source": row
        }

def main() -> None:
    if not ES_ENDPOINT or not ES_API_KEY:
        logger.error("Missing ES_ENDPOINT or ES_API_KEY environment variables.")
        sys.exit(3)

    if not CSV_PATH.exists():
        logger.error("CSV not found: %s", CSV_PATH.resolve())
        sys.exit(1)

    es = get_client()
    ensure_index(es, INDEX_NAME)

    logger.info("Reading rows from %s", CSV_PATH)
    rows_iter = read_csv_rows(CSV_PATH)
    actions = actions_from_rows(rows_iter, INDEX_NAME)

    logger.info("Starting bulk indexing into '%s' ...", INDEX_NAME)
    try:
        indexed, _ = helpers.bulk(
            es,
            actions,
            refresh=True,
            chunk_size=2000,
            request_timeout=120,
        )
        logger.info("Bulk indexing completed. Indexed actions: %s", indexed)
    except Exception as e:
        logger.exception("Bulk indexing failed: %s", e)
        sys.exit(2)

if __name__ == "__main__":
    main()

import os
import csv
import sys
import logging
import dotenv
from pathlib import Path
from typing import Iterator, Dict, Any, Optional
from elasticsearch import Elasticsearch, helpers

#  Config 
CSV_PATH = Path("data/IncidentTableData_cleaned.csv")
INDEX_NAME = "incident_tabledata"

# Elastic Cloud endpoint + API key
dotenv.load_dotenv()  # load from .env file
ES_ENDPOINT = os.getenv("ES_ENDPOINT")
ES_API_KEY = os.getenv("ES_API_KEY")

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("index_data")

#  Mappings 
MAPPINGS = {
    "mappings": {
        "dynamic": "strict",
        "properties": {
            "ticket_class": {"type": "keyword"},
            "ticket_priority": {"type": "integer"},
            "ticket_number": {"type": "keyword"},
            "ticket_status": {"type": "keyword"},
            "opened_date": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||strict_date_optional_time"},
            "hostname": {"type": "keyword"},
            "ticket_summary": {"type": "text"},
            "queue_id": {"type": "keyword"},
            "ticket_resolved_date": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||strict_date_optional_time"},
            "resolution_code": {"type": "keyword"},
            "resolution_text": {"type": "text"},
            "ticket_closed_date": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||strict_date_optional_time"},
            "call_code": {"type": "keyword"},
            "reported_by": {"type": "keyword"},
            "executed_automata": {"type": "keyword"},
            "recommended_automata": {"type": "keyword"},
            "actionable": {"type": "keyword"},
            "assignment_queue": {"type": "keyword"},
            "autogenerated": {"type": "keyword"},
            "automation_engine": {"type": "integer"},
            "business_application": {"type": "keyword"},
            "closure_code": {"type": "keyword"},
            "sub_category": {"type": "keyword"},
            "category": {"type": "keyword"},
            "alert_key": {"type": "keyword"},
            "opened_year": {"type": "integer"},
            "opened_month": {"type": "integer"},
            "opened_day": {"type": "integer"},
            "opened_hour": {"type": "integer"},
            "resolution_time_hours": {"type": "float"},
            "ticket_age_days": {"type": "integer"},
            "is_automated": {"type": "boolean"},
            "critical_application_flag": {"type": "boolean"},
        }
    }
}

ID_FIELDS = (
    "incident_ticket_number",
    "Incident Ticket Number",
    "incident ticket number",
    "ticket_number",
    "Ticket Number",
    "ticketnumber",
)

def get_client() -> Elasticsearch:
    es = Elasticsearch(
        ES_ENDPOINT,
        api_key=ES_API_KEY,
        request_timeout=120,
        retry_on_timeout=True,
        max_retries=3,
        verify_certs=True,
    )
    try:
        if not es.ping():
            logger.warning("Elasticsearch ping failed; check endpoint/API key.")
    except Exception as e:
        logger.warning("Ping raised an exception: %s", e)
    return es

def ensure_index(es: Elasticsearch, index_name: str) -> None:
    """Create index with explicit mappings if it doesn't exist."""
    try:
        exists = es.indices.exists(index=index_name)
    except Exception:
        exists = False
    if not exists:
        logger.info("Creating index '%s' with explicit mappings...", index_name)
        es.indices.create(index=index_name, body=MAPPINGS)
    else:
        logger.info("Index '%s' already exists; skipping creation.", index_name)

def sniff_delimiter(path: Path) -> str:
    """Auto-detect CSV delimiter; default to comma."""
    with path.open("r", encoding="utf-8-sig", newline="") as fh:
        sample = fh.read(8192)
        try:
            return csv.Sniffer().sniff(sample, delimiters=[",", "\t", ";", "|"]).delimiter
        except Exception:
            return ","

def read_csv_rows(path: Path) -> Iterator[Dict[str, Any]]:
    """
    Stream rows from CSV as dicts (CSV to JSON conversion).
    Auto-detects delimiter and handles BOM.
    """
    delimiter = sniff_delimiter(path)
    with path.open(newline="", encoding="utf-8-sig") as fh:
        reader = csv.DictReader(fh, delimiter=delimiter)
        if reader.fieldnames and len(reader.fieldnames) == 1 and "," in (reader.fieldnames[0] or ""):
            raise ValueError("Bad delimiter detected: header collapsed into one column.")
        for row in reader:
            yield row

def normalize_row(row: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert 'Unknown' and empty strings to None.
    Convert TRUE/FALSE/Y/N to booleans.
    Convert numeric fields to correct types.
    """
    out: Dict[str, Any] = {}
    for k, v in row.items():
        if isinstance(v, str):
            v_clean = v.strip()
            v_lower = v_clean.lower()
            if v_lower == "unknown" or v_clean == "":
                out[k] = None
            elif v_lower in {"true", "y", "yes"}:
                out[k] = True
            elif v_lower in {"false", "n", "no"}:
                out[k] = False
            else:
                if k in {"ticket_priority", "opened_year", "opened_month", "opened_day", "opened_hour", "ticket_age_days"}:
                    try: out[k] = int(v_clean)
                    except Exception: out[k] = None
                elif k in {"resolution_time_hours"}:
                    try: out[k] = float(v_clean)
                    except Exception: out[k] = None
                else:
                    out[k] = v_clean
        else:
            out[k] = v
    return out

def pick_doc_id(row: Dict[str, Any]) -> Optional[str]:
    lower_map = {k.lower(): k for k in row.keys() if k}
    for key in ID_FIELDS:
        lk = key.lower()
        if lk in lower_map:
            val = row.get(lower_map[lk])
            if val is not None:
                sid = str(val).strip()
                if sid:
                    return sid
    return None

def actions_from_rows(rows: Iterator[Dict[str, Any]], index_name: str):
    """
    Turn CSV rows into bulk index actions.
    Use incident ticket number (or ticket_number variants) as _id.
    """
    for row in rows:
        row = normalize_row(row)
        for alias in list(row.keys()):
            if alias.lower() in {"incident ticket number", "incident_ticket_number"}:
                row.setdefault("ticket_number", row[alias])
        doc_id = pick_doc_id(row)
        if not doc_id:
            logger.warning("Skipping row without incident ticket number.")
            continue
        yield {"_op_type": "index", "_index": index_name, "_id": doc_id, "_source": row}

def main() -> None:
    if not ES_ENDPOINT or not ES_API_KEY:
        logger.error("Missing ES_ENDPOINT or ES_API_KEY environment variables.")
        sys.exit(3)
    if not CSV_PATH.exists():
        logger.error("CSV not found: %s", CSV_PATH.resolve())
        sys.exit(1)

    es = get_client()
    ensure_index(es, INDEX_NAME)

    try:
        rows_iter = read_csv_rows(CSV_PATH)
    except Exception as e:
        logger.exception("CSV format/delimiter issue: %s", e)
        sys.exit(4)

    actions = actions_from_rows(rows_iter, INDEX_NAME)
    logger.info("Starting bulk indexing into '%s' ...", INDEX_NAME)
    try:
        indexed, _ = helpers.bulk(
            es,
            actions,
            refresh=True,
            chunk_size=2000,
            request_timeout=120,
        )
        logger.info("Bulk indexing completed. Indexed actions: %s", indexed)
    except Exception as e:
        logger.exception("Bulk indexing failed: %s", e)
        sys.exit(2)

if __name__ == "__main__":
    main()
